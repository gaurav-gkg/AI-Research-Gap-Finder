# AI Research Paper Analyzer - RAG with React & FastAPI

A modern web application that uses Retrieval-Augmented Generation (RAG) with Ollama to analyze research papers and extract key insights or identify research gaps.

## âœ¨ Features

- **ğŸ¨ Modern React UI**: Beautiful, responsive interface with smooth animations
- **ğŸ“„ PDF Analysis**: Upload any research paper in PDF format
- **ğŸ’¡ Key Insights Extraction**: Automatically identify and summarize main findings
- **ğŸ” Research Gaps Detection**: Highlight limitations and areas for future research
- **ğŸš€ FastAPI Backend**: High-performance REST API
- **ğŸ¤– Local AI Processing**: Uses Ollama for complete privacy and offline operation
- **ğŸ“± Responsive Design**: Works seamlessly on desktop, tablet, and mobile

## ğŸ› ï¸ Technology Stack

### Backend

- **FastAPI**: Modern, high-performance web framework
- **LangChain**: Framework for RAG applications
- **Ollama**: Local LLM inference (llama3.2 + nomic-embed-text)
- **FAISS**: Vector similarity search for document retrieval

### Frontend

- **React 18**: Modern UI library
- **Axios**: HTTP client
- **React Dropzone**: Drag & drop file uploads
- **React Markdown**: Render analysis results
- **Lucide React**: Beautiful icons
- **Framer Motion**: Smooth animations

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Node.js 16+ and npm
- Ollama installed and running

### Installation

#### Backend Setup

1. Clone the repository:

   ```bash
   git clone https://github.com/your-username/AI-Research-Analyzer.git
   cd AI-Research-Analyzer
   ```

2. Install Python dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Install and start Ollama:

   ```bash
   # Install Ollama from https://ollama.ai

   # Start Ollama service
   ollama serve

   # Pull required models (in another terminal)
   ollama pull llama3.2
   ollama pull nomic-embed-text
   ```

4. Start the FastAPI backend:
   ```bash
   python run.py
   ```
   The backend will run at http://localhost:8000

#### Frontend Setup

1. Navigate to frontend directory:

   ```bash
   cd frontend
   ```

2. Install npm dependencies:

   ```bash
   npm install
   ```

3. Start the React development server:
   ```bash
   npm start
   ```
   The frontend will run at http://localhost:3000

## ğŸ“– Usage

1. Open http://localhost:3000 in your browser
2. Drag & drop or click to upload a research paper PDF
3. Select analysis type:
   - **Key Insights**: Extract main findings and important points
   - **Research Gaps**: Identify limitations and future research directions
4. Click "Analyze Paper" and wait for AI analysis
5. View beautifully formatted results with markdown support

## ğŸ”§ API Endpoints

### POST /api/analyze

Analyze a research paper

**Request:**

- `file`: PDF file (multipart/form-data)
- `query_type`: "Key Insights" or "Research Gaps"

**Response:**

```json
{
  "response": "Markdown formatted analysis",
  "sources": "Source information",
  "success": true,
  "error": null
}
```

### GET /api/health

Health check endpoint

## ğŸ—ï¸ Architecture

The application uses Retrieval-Augmented Generation (RAG):

1. **Document Processing**: PDFs are loaded and split into chunks
2. **Vector Embedding**: Text chunks are converted to embeddings using Ollama
3. **Retrieval**: Relevant chunks are retrieved using FAISS vector search
4. **Generation**: Retrieved content is sent to Llama 3.2 with specialized prompts
5. **Response**: Structured markdown analysis is returned

## ğŸ¨ Design Features

- **Modern gradient background** (purple theme inspired by Dribbble)
- **Glass-morphism effects** on cards
- **Smooth animations** and transitions
- **Drag & drop file upload** with visual feedback
- **Real-time loading states** and error handling
- **Responsive design** for all screen sizes
- **Clean typography** using Inter font

## ğŸš€ Building for Production

```bash
# Build React frontend
cd frontend
npm run build

# The build files will be in frontend/build/
# Uncomment static file mounting in app/main.py to serve from backend
```

## ğŸ“ Project Structure

```
AI-Research-Analyzer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py          # FastAPI REST API
â”‚   â””â”€â”€ utils.py         # RAG processing
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.js
â”‚   â”‚   â”‚   â””â”€â”€ FileUpload.css
â”‚   â”‚   â”œâ”€â”€ App.js
â”‚   â”‚   â”œâ”€â”€ App.css
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ run.py
â””â”€â”€ requirements.txt
```

## ğŸ”® Future Improvements

- Multiple document comparison
- Export results to PDF/DOCX
- Chat interface for follow-up questions
- Support for more document formats
- User authentication and history
- Advanced visualization of findings

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

- Built with [LangChain](https://github.com/langchain-ai/langchain)
- Powered by Azure OpenAI models
- Interface created with [Gradio](https://gradio.app/)
